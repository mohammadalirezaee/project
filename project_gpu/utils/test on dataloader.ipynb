{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19350"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap = cv2.VideoCapture('D:/AmirKabir/tez/ewap_dataset/ewap_dataset/seq_hotel/seq_hotel.avi')\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "total_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def video_to_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    # Loop through all frames\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            break\n",
    "    # Release the video capture object\n",
    "    cap.release()\n",
    "    frames = np.array(frames)\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'D:/AmirKabir/tez/eighen trajectory/ewap_dataset/ewap_dataset/seq_hotel/seq_hotel.avi'\n",
    "frame = video_to_frames(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture('D:/AmirKabir/tez/eighen trajectory/ewap_dataset/ewap_dataset/seq_hotel/seq_hotel.avi')\n",
    "# Set the frame position to 10\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 19260)\n",
    "\n",
    "# Read the frame at position 10\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Check if the frame is valid\n",
    "if ret:\n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame 11', frame)\n",
    "    cv2.waitKey(0)\n",
    "else:\n",
    "    print(\"Error reading frame\")\n",
    "\n",
    "# Release the video capture object\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(576, 720, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.transforms import transforms\n",
    "from PIL import Image\n",
    "\n",
    "def prepare_image(frame):\n",
    "    transform = transforms.Compose([\n",
    "                      transforms.ToPILImage(),\n",
    "                      transforms.Resize((300, 300))\n",
    "                      ])\n",
    "    transformed_image = transform(frame)\n",
    "    return transformed_image\n",
    "\n",
    "def video_to_frames(video_path, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    frame_num = 0\n",
    "    # Loop through all frames\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            image = prepare_image(frame)\n",
    "            image.save(f\"{output_dir}/frame_{frame_num}.jpg\")\n",
    "            frame_num += 10\n",
    "        else:\n",
    "            break\n",
    "    # Release the video capture object\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara01.avi'\n",
    "output_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara01_frames'\n",
    "video_to_frames(video_path,output_dir)\n",
    "video_path = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara02.avi'\n",
    "output_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara02_frames'\n",
    "video_to_frames(video_path,output_dir)\n",
    "video_path = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/students003.avi'\n",
    "output_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/students003_frames'\n",
    "video_to_frames(video_path,output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_dataloader(data_dir, phase, obs_len, pred_len, batch_size):\n",
    "    r\"\"\"Get dataloader for a specific phase\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): path to the dataset directory\n",
    "        phase (str): phase of the data, one of 'train', 'val', 'test'\n",
    "        obs_len (int): length of observed trajectory\n",
    "        pred_len (int): length of predicted trajectory\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loader_phase (torch.utils.data.DataLoader): dataloader for the specific phase\n",
    "    \"\"\"\n",
    "\n",
    "    assert phase in ['train', 'val', 'test']\n",
    "    \n",
    "    \n",
    "    data_set = data_dir + '/' + phase + '/'\n",
    "    shuffle = True if phase == 'train' else False\n",
    "    drop_last = True if phase == 'train' else False \n",
    "    dataset_phase = TrajectoryDataset(data_set, obs_len=obs_len, pred_len=pred_len)\n",
    "    sampler_phase = None\n",
    "    if batch_size > 1:\n",
    "        sampler_phase = TrajBatchSampler(dataset_phase, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    loader_phase = DataLoader(dataset_phase, batch_sampler=sampler_phase ,collate_fn=traj_collate_fn, pin_memory=True)\n",
    "    return loader_phase\n",
    "\n",
    "\n",
    "def traj_collate_fn(data):\n",
    "    r\"\"\"Collate function for the dataloader\n",
    "\n",
    "    Args:\n",
    "        data (list): list of tuples of (obs_seq, pred_seq, non_linear_ped, loss_mask, seq_start_end)\n",
    "\n",
    "    Returns:\n",
    "        obs_seq_list (torch.Tensor): (num_ped, obs_len, 2)\n",
    "        pred_seq_list (torch.Tensor): (num_ped, pred_len, 2)\n",
    "        non_linear_ped_list (torch.Tensor): (num_ped,)\n",
    "        loss_mask_list (torch.Tensor): (num_ped, obs_len + pred_len)\n",
    "        scene_mask (torch.Tensor): (num_ped, num_ped)\n",
    "        seq_start_end (torch.Tensor): (num_ped, 2)\n",
    "        frame_list_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    obs_seq_list, pred_seq_list, non_linear_ped_list, loss_mask_list, _,frame_list_tensor= zip(*data)\n",
    "\n",
    "    _len = [len(seq) for seq in obs_seq_list]\n",
    "    cum_start_idx = [0] + np.cumsum(_len).tolist()\n",
    "    seq_start_end = [[start, end] for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "    seq_start_end = torch.LongTensor(seq_start_end)\n",
    "    scene_mask = torch.zeros(sum(_len), sum(_len), dtype=torch.bool)\n",
    "    for idx, (start, end) in enumerate(seq_start_end):\n",
    "        scene_mask[start:end, start:end] = 1\n",
    "\n",
    "    out = [torch.cat(obs_seq_list, dim=0), torch.cat(pred_seq_list, dim=0),\n",
    "           torch.cat(non_linear_ped_list, dim=0), torch.cat(loss_mask_list, dim=0), scene_mask, seq_start_end,frame_list_tensor]\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "class TrajBatchSampler(Sampler):\n",
    "    r\"\"\"Samples batched elements by yielding a mini-batch of indices.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size=64, shuffle=False, drop_last=False, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert len(self.data_source) == len(self.data_source.num_peds_in_seq)\n",
    "\n",
    "        if self.shuffle:\n",
    "            if self.generator is None:\n",
    "                generator = torch.Generator()\n",
    "                generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "            else:\n",
    "                generator = self.generator\n",
    "            indices = torch.randperm(len(self.data_source), generator=generator).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.data_source)))\n",
    "        num_peds_indices = self.data_source.num_peds_in_seq[indices]\n",
    "\n",
    "        batch = []\n",
    "        total_num_peds = 0\n",
    "        for idx, num_peds in zip(indices, num_peds_indices):\n",
    "            batch.append(idx)\n",
    "            total_num_peds += num_peds\n",
    "            if total_num_peds >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                total_num_peds = 0\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Approximated number of batches.\n",
    "        # The order of trajectories can be shuffled, so this number can vary from run to run.\n",
    "        if self.drop_last:\n",
    "            return sum(self.data_source.num_peds_in_seq) // self.batch_size\n",
    "        else:\n",
    "            return (sum(self.data_source.num_peds_in_seq) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            line = [float(i) for i in line]\n",
    "            data.append(line)\n",
    "    return np.asarray(data)\n",
    "\n",
    "\n",
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def prepare_image(frame):\n",
    "    transform = transforms.Compose([\n",
    "                      transforms.ToPILImage(),\n",
    "                      transforms.Resize(300),\n",
    "                      transforms.ToTensor()])\n",
    "    transformed_image = transform(frame)\n",
    "    transformed_image = transformed_image.unsqueeze(0)\n",
    "    return transformed_image.to(device)\n",
    "\n",
    "\n",
    "\n",
    "# In your TrajectoryDataset class\n",
    "\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.02, min_ped=1, delim='\\t'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format <frame_id> <ped_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non-linear traj when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a sequence\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        all_file = os.listdir(self.data_dir)\n",
    "        all_file = [os.path.join(self.data_dir, _path) for _path in all_file]\n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        frame_list = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        pedestrain_id = []#new add\n",
    "        text_format = 'txt'\n",
    "\n",
    "        for name in all_file:\n",
    "            path = None\n",
    "            frame_path = None\n",
    "            if name[-3:] == text_format:\n",
    "                path = name \n",
    "                frame_path = name[:-4] + '_frames/'\n",
    " \n",
    "                try:\n",
    "                    data = read_file(path, delim)\n",
    "                    video_path = frame_path\n",
    "                except:\n",
    "                    data = read_file(path, delim = 'space')    \n",
    "                    video_path = frame_path\n",
    "                    \n",
    "                frame_id = [] #new add    \n",
    "                frames = np.unique(data[:, 0]).tolist()\n",
    "                frame_data = []\n",
    "                for frame in frames:\n",
    "                    frame_data.append(data[frame == data[:, 0], :])\n",
    "                num_sequences = int(math.ceil((len(frames) - self.seq_len + 1) / skip))\n",
    "\n",
    "                for idx in range(0, num_sequences * self.skip + 1, skip):\n",
    "                    curr_seq_data = np.concatenate(frame_data[idx:idx + self.seq_len], axis=0)\n",
    "                    frame_range = np.unique(curr_seq_data[:, 0])       #new add\n",
    "                    peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "                    curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n",
    "                    curr_loss_mask = np.zeros((len(peds_in_curr_seq), self.seq_len))\n",
    "                    num_peds_considered = 0\n",
    "                    _non_linear_ped = []\n",
    "                    ped_list = [] #new add\n",
    "                    for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "                        curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "                        curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                        pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                        pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                        if pad_end - pad_front != self.seq_len:\n",
    "                            continue\n",
    "                        curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                        curr_ped_seq = curr_ped_seq\n",
    "                        _idx = num_peds_considered\n",
    "                        curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "                        # Linear vs Non-Linear Trajectory\n",
    "                        _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                        curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                        num_peds_considered += 1\n",
    "                        ped_list.append(ped_id) #new add\n",
    "\n",
    "                    if num_peds_considered > min_ped:\n",
    "                        non_linear_ped += _non_linear_ped\n",
    "                        num_peds_in_seq.append(num_peds_considered)\n",
    "                        loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                        seq_list.append(curr_seq[:num_peds_considered])\n",
    "                        frame_path = [video_path + 'frame_' + str(int(number)) + '.jpg' for number in frame_range][:8]\n",
    "                        temp = [plt.imread(path) for path in frame_path]\n",
    "                        frame_list.append(temp)\n",
    "                        frame_id.append(frame_range) #new add\n",
    "                        pedestrain_id.append(ped_list) #new add\n",
    "                    \n",
    "            \n",
    "            # cap = cv2.VideoCapture(self.frame_path + '/seq_hotel.avi')\n",
    "            # frame_lists = [(row.min() , row.max())  for row in frame_id] #new add\n",
    "            # images_list = []\n",
    "            # for tup in frame_lists:\n",
    "            #     temp_list = []\n",
    "            #     current_frame_number = tup[0]\n",
    "            #     for i in range(obs_len):\n",
    "            #         cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame_number)\n",
    "            #         ret, current_frame = cap.read()\n",
    "            #         frame = prepare_image(current_frame).to(device)\n",
    "            #         temp_list.append(frame)\n",
    "            #         current_frame_number+=10\n",
    "            #     images_list.append(temp_list)\n",
    "            # import pickle\n",
    "\n",
    "            # Specify the full path to save the images_list as a pickle file\n",
    "            # save_path = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/image_List_hotel/images_list.pkl'\n",
    "            # with open(save_path, 'wb') as f:\n",
    "            #     pickle.dump(images_list, f)\n",
    "            \n",
    "        # with open(save_path, 'rb') as f:\n",
    "        # self.frame_lists = [(row.min(), row.max()) for row in frame_id]\n",
    "        # self.images = extract_frames(video_path, frame_lists, self.obs_len)\n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        # frame_list = np.concatenate(frame_list, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "         #new add\n",
    "        self.pedestrain_id = np.concatenate(pedestrain_id , axis = 0) #new add\n",
    "        self.num_peds_in_seq = np.array(num_peds_in_seq) \n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.frame_list = frame_list\n",
    "        self.obs_traj = torch.from_numpy(seq_list[:, :, :self.obs_len]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.pred_traj = torch.from_numpy(seq_list[:, :, self.obs_len:]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "    \n",
    "    # def extract_frames(self, video_path, min_frame, max_frame, obs_len):\n",
    "    #     images_list = []\n",
    "    #     cap =  cv2.VideoCapture(video_path) \n",
    "    #     temp_list = []\n",
    "    #     current_frame_number = min_frame\n",
    "    #     for _ in range(obs_len):\n",
    "    #         cap.set(cv2.CAP_PROP_POS_FRAMES, current_frame_number)\n",
    "    #         ret, current_frame = cap.read()\n",
    "    #         if not ret:\n",
    "    #             raise RuntimeError(f\"Could not read frame {current_frame_number} from {video_path}\")\n",
    "    #         frame = prepare_image(current_frame).to(device)\n",
    "    #         temp_list.append(frame)\n",
    "    #         current_frame_number += 10\n",
    "    #     images_list.append(temp_list)\n",
    "    #     return images_list\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        # min_frame, max_frame= self.frame_lists[index]\n",
    "        # frames = self.extract_frames(self.video_path, min_frame, max_frame, self.obs_len)\n",
    "        out = [self.obs_traj[start:end], self.pred_traj[start:end],\n",
    "               self.non_linear_ped[start:end], self.loss_mask[start:end], [[0, end - start]], self.frame_list[index]]\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_dataloader(data_dir, phase, obs_len, pred_len, batch_size):\n",
    "    r\"\"\"Get dataloader for a specific phase\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): path to the dataset directory\n",
    "        phase (str): phase of the data, one of 'train', 'val', 'test'\n",
    "        obs_len (int): length of observed trajectory\n",
    "        pred_len (int): length of predicted trajectory\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loader_phase (torch.utils.data.DataLoader): dataloader for the specific phase\n",
    "    \"\"\"\n",
    "\n",
    "    assert phase in ['train', 'val', 'test']\n",
    "    \n",
    "    \n",
    "    data_set = data_dir + '/' + phase + '/'\n",
    "    shuffle = True if phase == 'train' else False\n",
    "    drop_last = True if phase == 'train' else False \n",
    "    dataset_phase = TrajectoryDataset(data_set, obs_len=obs_len, pred_len=pred_len)\n",
    "    sampler_phase = None\n",
    "    if batch_size > 1:\n",
    "        sampler_phase = TrajBatchSampler(dataset_phase, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    loader_phase = DataLoader(dataset_phase, batch_sampler=sampler_phase ,collate_fn=traj_collate_fn, pin_memory=True)\n",
    "    return loader_phase\n",
    "\n",
    "\n",
    "def traj_collate_fn(data):\n",
    "    r\"\"\"Collate function for the dataloader\n",
    "\n",
    "    Args:\n",
    "        data (list): list of tuples of (obs_seq, pred_seq, non_linear_ped, loss_mask, seq_start_end)\n",
    "\n",
    "    Returns:\n",
    "        obs_seq_list (torch.Tensor): (num_ped, obs_len, 2)\n",
    "        pred_seq_list (torch.Tensor): (num_ped, pred_len, 2)\n",
    "        non_linear_ped_list (torch.Tensor): (num_ped,)\n",
    "        loss_mask_list (torch.Tensor): (num_ped, obs_len + pred_len)\n",
    "        scene_mask (torch.Tensor): (num_ped, num_ped)\n",
    "        seq_start_end (torch.Tensor): (num_ped, 2)\n",
    "        frame_list_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    obs_seq_list, pred_seq_list, non_linear_ped_list, loss_mask_list, _,frame_list_tensor= zip(*data)\n",
    "\n",
    "    _len = [len(seq) for seq in obs_seq_list]\n",
    "    cum_start_idx = [0] + np.cumsum(_len).tolist()\n",
    "    seq_start_end = [[start, end] for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "    seq_start_end = torch.LongTensor(seq_start_end)\n",
    "    scene_mask = torch.zeros(sum(_len), sum(_len), dtype=torch.bool)\n",
    "    for idx, (start, end) in enumerate(seq_start_end):\n",
    "        scene_mask[start:end, start:end] = 1\n",
    "\n",
    "    out = [torch.cat(obs_seq_list, dim=0), torch.cat(pred_seq_list, dim=0),\n",
    "           torch.cat(non_linear_ped_list, dim=0), torch.cat(loss_mask_list, dim=0), scene_mask, seq_start_end,frame_list_tensor]\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "class TrajBatchSampler(Sampler):\n",
    "    r\"\"\"Samples batched elements by yielding a mini-batch of indices.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size=64, shuffle=False, drop_last=False, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert len(self.data_source) == len(self.data_source.num_peds_in_seq)\n",
    "\n",
    "        if self.shuffle:\n",
    "            if self.generator is None:\n",
    "                generator = torch.Generator()\n",
    "                generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "            else:\n",
    "                generator = self.generator\n",
    "            indices = torch.randperm(len(self.data_source), generator=generator).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.data_source)))\n",
    "        num_peds_indices = self.data_source.num_peds_in_seq[indices]\n",
    "\n",
    "        batch = []\n",
    "        total_num_peds = 0\n",
    "        for idx, num_peds in zip(indices, num_peds_indices):\n",
    "            batch.append(idx)\n",
    "            total_num_peds += num_peds\n",
    "            if total_num_peds >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                total_num_peds = 0\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Approximated number of batches.\n",
    "        # The order of trajectories can be shuffled, so this number can vary from run to run.\n",
    "        if self.drop_last:\n",
    "            return sum(self.data_source.num_peds_in_seq) // self.batch_size\n",
    "        else:\n",
    "            return (sum(self.data_source.num_peds_in_seq) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            line = [float(i) for i in line]\n",
    "            data.append(line)\n",
    "    return np.asarray(data)\n",
    "\n",
    "\n",
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def prepare_image(frame):\n",
    "    transform = transforms.Compose([\n",
    "                      transforms.ToPILImage(),\n",
    "                      transforms.Resize(300),\n",
    "                      transforms.ToTensor()])\n",
    "    transformed_image = transform(frame)\n",
    "    transformed_image = transformed_image.unsqueeze(0)\n",
    "    return transformed_image.to(device)\n",
    "\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.02, min_ped=1, delim='\\t'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format <frame_id> <ped_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non-linear traj when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a sequence\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        all_file = os.listdir(self.data_dir)\n",
    "        all_file = [os.path.join(self.data_dir, _path) for _path in all_file]\n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        frame_list = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        pedestrain_id = []#new add\n",
    "        text_format = 'txt'\n",
    "\n",
    "        for name in all_file:\n",
    "            path = None\n",
    "            frame_path = None\n",
    "            if name[-3:] == text_format:\n",
    "                path = name \n",
    "                frame_path = name[:-4] + '_frames/'\n",
    " \n",
    "                try:\n",
    "                    data = read_file(path, delim)\n",
    "                    video_path = frame_path\n",
    "                except:\n",
    "                    data = read_file(path, delim = 'space')    \n",
    "                    video_path = frame_path\n",
    "                    \n",
    "                frame_id = [] #new add    \n",
    "                frames = np.unique(data[:, 0]).tolist()\n",
    "                frame_data = []\n",
    "                for frame in frames:\n",
    "                    frame_data.append(data[frame == data[:, 0], :])\n",
    "                num_sequences = int(math.ceil((len(frames) - self.seq_len + 1) / skip))\n",
    "\n",
    "                for idx in range(0, num_sequences * self.skip + 1, skip):\n",
    "                    curr_seq_data = np.concatenate(frame_data[idx:idx + self.seq_len], axis=0)\n",
    "                    frame_range = np.unique(curr_seq_data[:, 0])       #new add\n",
    "                    peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "                    curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n",
    "                    curr_loss_mask = np.zeros((len(peds_in_curr_seq), self.seq_len))\n",
    "                    num_peds_considered = 0\n",
    "                    _non_linear_ped = []\n",
    "                    ped_list = [] #new add\n",
    "                    for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "                        curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "                        curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                        pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                        pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                        if pad_end - pad_front != self.seq_len:\n",
    "                            continue\n",
    "                        curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                        curr_ped_seq = curr_ped_seq\n",
    "                        _idx = num_peds_considered\n",
    "                        curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "                        # Linear vs Non-Linear Trajectory\n",
    "                        _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                        curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                        num_peds_considered += 1\n",
    "                        ped_list.append(ped_id) #new add\n",
    "\n",
    "                    if num_peds_considered > min_ped:\n",
    "                        non_linear_ped += _non_linear_ped\n",
    "                        num_peds_in_seq.append(num_peds_considered)\n",
    "                        loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                        seq_list.append(curr_seq[:num_peds_considered])\n",
    "                        frame_path = [video_path + 'frame_' + str(int(number)) + '.jpg' for number in frame_range][:8]\n",
    "                        temp = [plt.imread(path) for path in frame_path]\n",
    "                        frame_list.append(temp)\n",
    "                        frame_id.append(frame_range) #new add\n",
    "                        pedestrain_id.append(ped_list) #new add\n",
    "                    \n",
    "            \n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "         #new add\n",
    "        self.pedestrain_id = np.concatenate(pedestrain_id , axis = 0) #new add\n",
    "        self.num_peds_in_seq = np.array(num_peds_in_seq) \n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.frame_list = frame_list\n",
    "        self.obs_traj = torch.from_numpy(seq_list[:, :, :self.obs_len]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.pred_traj = torch.from_numpy(seq_list[:, :, self.obs_len:]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [self.obs_traj[start:end], self.pred_traj[start:end],\n",
    "               self.non_linear_ped[start:end], self.loss_mask[start:end], [[0, end - start]], self.frame_list[index]]\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/biwi_hotel_train.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/biwi_hotel_train_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara01.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara01_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara02.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/crowds_zara02_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/students003.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/students003_frames']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/train/'\n",
    "all_file = os.listdir(dataset_dir)\n",
    "# all_files = [p for p in all_file if p[-3:] == text_format]\n",
    "all_file = [os.path.join(dataset_dir, _path) for _path in all_file]\n",
    "all_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_test_cpu/datasets/eth/'\n",
    "loader_train = get_dataloader(dataset_dir, 'train', 8, 12 ,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cnt, batch in enumerate(loader_train, desc=f'Train Epoch {epoch}', mininterval=1):\n",
    "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            obs_traj, pred_traj = [tensor.to(device, non_blocking=True) for tensor in batch[:2]]\n",
    "            frames = [[tensor.to(device, non_blocking=True) for tensor in sublist] for sublist in batch[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = iter(loader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = next(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[-1][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [torch.from_numpy(np.copy(sublist)).to(device, non_blocking=True) for sublist in test[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[6][0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for data  in loader_train:\n",
    "    all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in loader_train:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    obs_traj, pred_traj = [tensor.to(device, non_blocking=True) for tensor in batch[:2]]\n",
    "    frames = [[tensor.to(device, non_blocking=True) for tensor in sublist] for sublist in batch[-1][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            line = [float(i) for i in line]\n",
    "            data.append(line)\n",
    "    return np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_peds_in_seq = []\n",
    "seq_list = []\n",
    "loss_mask_list = []\n",
    "non_linear_ped = []\n",
    "frame_id = []\n",
    "path = 'D:/AmirKabir/tez/eighen trajectory/nuscenes_test_cpu/nuscenes_test_cpu/datasets/eth/train/biwi_hotel_train.txt'\n",
    "try:\n",
    "    data = read_file(path)\n",
    "except:\n",
    "    data = read_file(path, delim = 'space')\n",
    "        \n",
    "frames = np.unique(data[:, 0]).tolist()\n",
    "frame_data = []\n",
    "for frame in frames:\n",
    "    frame_data.append(data[frame == data[:, 0], :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "obs_len = 8\n",
    "skip = 1\n",
    "pred_len = 12\n",
    "threshold=0.02\n",
    "min_ped=1\n",
    "num_sequences = int(math.ceil((len(frames) - seq_len + 1) / skip))\n",
    "num_peds_in_seq = []\n",
    "seq_list = []\n",
    "loss_mask_list = []\n",
    "non_linear_ped = []\n",
    "frame_id = []\n",
    "pedestrain_id = []\n",
    "for idx in range(0, num_sequences * skip + 1, skip):\n",
    "    curr_seq_data = np.concatenate(frame_data[idx:idx + seq_len], axis=0)\n",
    "    peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "    frame_range = np.unique(curr_seq_data[:, 0])\n",
    "    curr_seq = np.zeros((len(peds_in_curr_seq), 2, seq_len))\n",
    "    curr_loss_mask = np.zeros((len(peds_in_curr_seq), seq_len))\n",
    "    num_peds_considered = 0\n",
    "    _non_linear_ped = []\n",
    "    ped_list = []\n",
    "    for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "        curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "        curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "        pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "        pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "        if pad_end - pad_front != seq_len:\n",
    "            continue\n",
    "        curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "        curr_ped_seq = curr_ped_seq\n",
    "        _idx = num_peds_considered\n",
    "        curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "        # Linear vs Non-Linear Trajectory\n",
    "        _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "        curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "        num_peds_considered += 1\n",
    "        ped_list.append(ped_id)\n",
    "\n",
    "\n",
    "    if num_peds_considered > min_ped:\n",
    "        non_linear_ped += _non_linear_ped\n",
    "        num_peds_in_seq.append(num_peds_considered)\n",
    "        loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "        seq_list.append(curr_seq[:num_peds_considered])\n",
    "        frame_id.append(frame_range)\n",
    "        pedestrain_id.append(ped_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_seq = len(seq_list)\n",
    "seq_list = np.concatenate(seq_list, axis=0)\n",
    "loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "frame_lists = [(row.min() , row.max())  for row in frame_id]\n",
    "non_linear_ped = np.asarray(non_linear_ped)\n",
    "num_peds_in_seq = np.array(num_peds_in_seq)\n",
    "pedestrain_id = np.concatenate(pedestrain_id , axis = 0)\n",
    "# Convert numpy -> Torch T\n",
    "obs_traj = torch.from_numpy(seq_list[:, :, :obs_len]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "pred_traj = torch.from_numpy(seq_list[:, :, obs_len:]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "seq_start_end = [(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 5, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5,\n",
       "       5, 5, 5, 3, 4, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 2, 2, 2, 3, 2, 2, 2, 5, 5, 5, 3, 3,\n",
       "       5, 5, 5, 4, 5, 3, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 4, 4, 4, 3, 3,\n",
       "       2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2,\n",
       "       2, 2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 6, 6, 6, 6, 8, 7, 5, 5, 5])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_peds_in_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 8, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_traj_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_dir = \"D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/\"\n",
    "all_file = os.listdir(data_dir)\n",
    "all_file = [os.path.join(data_dir, _path) for _path in all_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/biwi_hotel_train.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/biwi_hotel_train_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/crowds_zara01.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/crowds_zara01_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/crowds_zara02.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/crowds_zara02_frames',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/students003.txt',\n",
       " 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/students003_frames']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/crowds_zara01_frames'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/train/crowds_zara01.txt'\n",
    "D:\\AmirKabir\\tez\\eighen trajectory\\project_git\\project\\project_gpu\\datasets\\hotel\\train\n",
    "splitted_path = path.split('/')\n",
    "address_until_datasets = '/'.join(splitted_path[:8])\n",
    "videos_name = splitted_path[-1][:-4] + '_frames'\n",
    "frame_path = address_until_datasets + '/' + videos_name\n",
    "frame_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def get_dataloader(data_dir, phase, obs_len, pred_len, batch_size):\n",
    "    r\"\"\"Get dataloader for a specific phase\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): path to the dataset directory\n",
    "        phase (str): phase of the data, one of 'train', 'val', 'test'\n",
    "        obs_len (int): length of observed trajectory\n",
    "        pred_len (int): length of predicted trajectory\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loader_phase (torch.utils.data.DataLoader): dataloader for the specific phase\n",
    "    \"\"\"\n",
    "\n",
    "    assert phase in ['train', 'val', 'test']\n",
    "    \n",
    "    \n",
    "    data_set = data_dir + '/' + phase + '/'\n",
    "    shuffle = True if phase == 'train' else False\n",
    "    drop_last = True if phase == 'train' else False \n",
    "    dataset_phase = TrajectoryDataset(data_set, obs_len=obs_len, pred_len=pred_len)\n",
    "    sampler_phase = None\n",
    "    if batch_size > 1:\n",
    "        sampler_phase = TrajBatchSampler(dataset_phase, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    loader_phase = DataLoader(dataset_phase, batch_sampler=sampler_phase ,collate_fn=traj_collate_fn, pin_memory=True)\n",
    "    return loader_phase\n",
    "\n",
    "\n",
    "def traj_collate_fn(data):\n",
    "    r\"\"\"Collate function for the dataloader\n",
    "\n",
    "    Args:\n",
    "        data (list): list of tuples of (obs_seq, pred_seq, non_linear_ped, loss_mask, seq_start_end)\n",
    "\n",
    "    Returns:\n",
    "        obs_seq_list (torch.Tensor): (num_ped, obs_len, 2)\n",
    "        pred_seq_list (torch.Tensor): (num_ped, pred_len, 2)\n",
    "        non_linear_ped_list (torch.Tensor): (num_ped,)\n",
    "        loss_mask_list (torch.Tensor): (num_ped, obs_len + pred_len)\n",
    "        scene_mask (torch.Tensor): (num_ped, num_ped)\n",
    "        seq_start_end (torch.Tensor): (num_ped, 2)\n",
    "        frame_list_tensor\n",
    "    \"\"\"\n",
    "\n",
    "    obs_seq_list, pred_seq_list, non_linear_ped_list, loss_mask_list, _,frame_list_tensor= zip(*data)\n",
    "\n",
    "    _len = [len(seq) for seq in obs_seq_list]\n",
    "    cum_start_idx = [0] + np.cumsum(_len).tolist()\n",
    "    seq_start_end = [[start, end] for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "    seq_start_end = torch.LongTensor(seq_start_end)\n",
    "    scene_mask = torch.zeros(sum(_len), sum(_len), dtype=torch.bool)\n",
    "    for idx, (start, end) in enumerate(seq_start_end):\n",
    "        scene_mask[start:end, start:end] = 1\n",
    "\n",
    "    out = [torch.cat(obs_seq_list, dim=0), torch.cat(pred_seq_list, dim=0),\n",
    "           torch.cat(non_linear_ped_list, dim=0), torch.cat(loss_mask_list, dim=0), scene_mask, seq_start_end,frame_list_tensor]\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "class TrajBatchSampler(Sampler):\n",
    "    r\"\"\"Samples batched elements by yielding a mini-batch of indices.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size=64, shuffle=False, drop_last=False, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert len(self.data_source) == len(self.data_source.num_peds_in_seq)\n",
    "\n",
    "        if self.shuffle:\n",
    "            if self.generator is None:\n",
    "                generator = torch.Generator()\n",
    "                generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "            else:\n",
    "                generator = self.generator\n",
    "            indices = torch.randperm(len(self.data_source), generator=generator).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.data_source)))\n",
    "        num_peds_indices = self.data_source.num_peds_in_seq[indices]\n",
    "\n",
    "        batch = []\n",
    "        total_num_peds = 0\n",
    "        for idx, num_peds in zip(indices, num_peds_indices):\n",
    "            batch.append(idx)\n",
    "            total_num_peds += num_peds\n",
    "            if total_num_peds >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                total_num_peds = 0\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Approximated number of batches.\n",
    "        # The order of trajectories can be shuffled, so this number can vary from run to run.\n",
    "        if self.drop_last:\n",
    "            return sum(self.data_source.num_peds_in_seq) // self.batch_size\n",
    "        else:\n",
    "            return (sum(self.data_source.num_peds_in_seq) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            line = [float(i) for i in line]\n",
    "            data.append(line)\n",
    "    return np.asarray(data)\n",
    "\n",
    "\n",
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "def prepare_image(frame):\n",
    "    transform = transforms.Compose([\n",
    "                      transforms.ToPILImage(),\n",
    "                      transforms.Resize(300),\n",
    "                      transforms.ToTensor()])\n",
    "    transformed_image = transform(frame)\n",
    "    transformed_image = transformed_image.unsqueeze(0)\n",
    "    return transformed_image.to(device)\n",
    "\n",
    "\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.02, min_ped=1, delim='\\t'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format <frame_id> <ped_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non-linear traj when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a sequence\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        all_file = os.listdir(self.data_dir)\n",
    "        all_file = [os.path.join(self.data_dir, _path) for _path in all_file]\n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        frame_list = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        pedestrain_id = []#new add\n",
    "        text_format = 'txt'\n",
    "\n",
    "        for path in all_file:\n",
    "            splitted_path = path.split('/')\n",
    "            address_until_datasets = '/'.join(splitted_path[:8])\n",
    "            videos_name = splitted_path[-1][:-4] + '_frames'\n",
    "            frame_path = address_until_datasets + '/' + videos_name\n",
    "\n",
    "            try:\n",
    "                data = read_file(path, delim)\n",
    "                video_path = frame_path\n",
    "            except:\n",
    "                data = read_file(path, delim = 'space')    \n",
    "                video_path = frame_path\n",
    "                \n",
    "            frame_id = [] #new add    \n",
    "            frames = np.unique(data[:, 0]).tolist()\n",
    "            frame_data = []\n",
    "            for frame in frames:\n",
    "                frame_data.append(data[frame == data[:, 0], :])\n",
    "            num_sequences = int(math.ceil((len(frames) - self.seq_len + 1) / skip))\n",
    "\n",
    "            for idx in range(0, num_sequences * self.skip + 1, skip):\n",
    "                curr_seq_data = np.concatenate(frame_data[idx:idx + self.seq_len], axis=0)\n",
    "                frame_range = np.unique(curr_seq_data[:, 0])       #new add\n",
    "                peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "                curr_seq = np.zeros((len(peds_in_curr_seq), 2, self.seq_len))\n",
    "                curr_loss_mask = np.zeros((len(peds_in_curr_seq), self.seq_len))\n",
    "                num_peds_considered = 0\n",
    "                _non_linear_ped = []\n",
    "                ped_list = [] #new add\n",
    "                for _, ped_id in enumerate(peds_in_curr_seq):\n",
    "                    curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "                    curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                    pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                    pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                    if pad_end - pad_front != self.seq_len:\n",
    "                        continue\n",
    "                    curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                    curr_ped_seq = curr_ped_seq\n",
    "                    _idx = num_peds_considered\n",
    "                    curr_seq[_idx, :, pad_front:pad_end] = curr_ped_seq\n",
    "                    # Linear vs Non-Linear Trajectory\n",
    "                    _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                    curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                    num_peds_considered += 1\n",
    "                    ped_list.append(ped_id) #new add\n",
    "\n",
    "                if num_peds_considered > min_ped:\n",
    "                    non_linear_ped += _non_linear_ped\n",
    "                    num_peds_in_seq.append(num_peds_considered)\n",
    "                    loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                    seq_list.append(curr_seq[:num_peds_considered])\n",
    "                    frame_path = [video_path +'/' +'frame_' + str(int(number)) + '.jpg' for number in frame_range][:8]\n",
    "                    temp = [plt.imread(path) for path in frame_path]\n",
    "                    frame_list.append(temp)\n",
    "                    frame_id.append(frame_range) #new add\n",
    "                    pedestrain_id.append(ped_list) #new add\n",
    "                    \n",
    "            \n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "         #new add\n",
    "        self.pedestrain_id = np.concatenate(pedestrain_id , axis = 0) #new add\n",
    "        self.num_peds_in_seq = np.array(num_peds_in_seq) \n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.frame_list = frame_list\n",
    "        self.obs_traj = torch.from_numpy(seq_list[:, :, :self.obs_len]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.pred_traj = torch.from_numpy(seq_list[:, :, self.obs_len:]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [self.obs_traj[start:end], self.pred_traj[start:end],\n",
    "               self.non_linear_ped[start:end], self.loss_mask[start:end], [[0, end - start]], self.frame_list[index]]\n",
    "        return out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = 'D:/AmirKabir/tez/eighen trajectory/project_git/project/project_gpu/datasets/eth/'\n",
    "loader_train = get_dataloader(dataset_dir, 'train', 8, 12 ,batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './datasets/hotel//train/biwi_eth.txt'\n",
    "splitted_path = path.split('/')\n",
    "address_until_datasets = '/'.join(splitted_path[:2])\n",
    "videos_name = splitted_path[-1][:-4] + '_frames'\n",
    "frame_path = address_until_datasets + '/' + videos_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./datasets/biwi_eth_frames'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
