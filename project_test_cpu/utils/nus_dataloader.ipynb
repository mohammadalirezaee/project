{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torch.utils.data.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(data_dir, phase, obs_len, pred_len, batch_size):\n",
    "    r\"\"\"Get dataloader for a specific phase\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): path to the dataset directory\n",
    "        phase (str): phase of the data, one of 'train', 'val', 'test'\n",
    "        obs_len (int): length of observed trajectory\n",
    "        pred_len (int): length of predicted trajectory\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        loader_phase (torch.utils.data.DataLoader): dataloader for the specific phase\n",
    "    \"\"\"\n",
    "\n",
    "    assert phase in ['train', 'val', 'test']\n",
    "\n",
    "    data_set = data_dir + '/' + phase + '/'\n",
    "    shuffle = True if phase == 'train' else False\n",
    "    drop_last = True if phase == 'train' else False\n",
    "\n",
    "    dataset_phase = TrajectoryDataset(data_set, obs_len=obs_len, pred_len=pred_len)\n",
    "    sampler_phase = None\n",
    "    if batch_size > 1:\n",
    "        sampler_phase = TrajBatchSampler(dataset_phase, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "    loader_phase = DataLoader(dataset_phase, collate_fn=traj_collate_fn, batch_sampler=sampler_phase, pin_memory=True)\n",
    "    return loader_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traj_collate_fn(data):\n",
    "    r\"\"\"Collate function for the dataloader\n",
    "\n",
    "    Args:\n",
    "        data (list): list of tuples of (obs_seq, pred_seq, non_linear_ped, loss_mask, seq_start_end)\n",
    "\n",
    "    Returns:\n",
    "        obs_seq_list (torch.Tensor): (num_ped, obs_len, 2)\n",
    "        pred_seq_list (torch.Tensor): (num_ped, pred_len, 2)\n",
    "        non_linear_ped_list (torch.Tensor): (num_ped,)\n",
    "        loss_mask_list (torch.Tensor): (num_ped, obs_len + pred_len)\n",
    "        scene_mask (torch.Tensor): (num_ped, num_ped)\n",
    "        seq_start_end (torch.Tensor): (num_ped, 2)\n",
    "    \"\"\"\n",
    "\n",
    "    obs_seq_list, pred_seq_list, non_linear_ped_list, loss_mask_list, _, _ = zip(*data)\n",
    "\n",
    "    _len = [len(seq) for seq in obs_seq_list]\n",
    "    cum_start_idx = [0] + np.cumsum(_len).tolist()\n",
    "    seq_start_end = [[start, end] for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "    seq_start_end = torch.LongTensor(seq_start_end)\n",
    "    scene_mask = torch.zeros(sum(_len), sum(_len), dtype=torch.bool)\n",
    "    for idx, (start, end) in enumerate(seq_start_end):\n",
    "        scene_mask[start:end, start:end] = 1\n",
    "\n",
    "    out = [torch.cat(obs_seq_list, dim=0), torch.cat(pred_seq_list, dim=0),\n",
    "           torch.cat(non_linear_ped_list, dim=0), torch.cat(loss_mask_list, dim=0), scene_mask, seq_start_end]\n",
    "    return tuple(out)\n",
    "\n",
    "\n",
    "class TrajBatchSampler(Sampler):\n",
    "    r\"\"\"Samples batched elements by yielding a mini-batch of indices.\n",
    "    Args:\n",
    "        data_source (Dataset): dataset to sample from\n",
    "        batch_size (int): Size of mini-batch.\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        drop_last (bool): If ``True``, the sampler will drop the last batch if\n",
    "            its size would be less than ``batch_size``\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source, batch_size=64, shuffle=False, drop_last=False, generator=None):\n",
    "        self.data_source = data_source\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.drop_last = drop_last\n",
    "        self.generator = generator\n",
    "\n",
    "    def __iter__(self):\n",
    "        assert len(self.data_source) == len(self.data_source.num_peds_in_seq)\n",
    "\n",
    "        if self.shuffle:\n",
    "            if self.generator is None:\n",
    "                generator = torch.Generator()\n",
    "                generator.manual_seed(int(torch.empty((), dtype=torch.int64).random_().item()))\n",
    "            else:\n",
    "                generator = self.generator\n",
    "            indices = torch.randperm(len(self.data_source), generator=generator).tolist()\n",
    "        else:\n",
    "            indices = list(range(len(self.data_source)))\n",
    "        num_peds_indices = self.data_source.num_peds_in_seq[indices]\n",
    "\n",
    "        batch = []\n",
    "        total_num_peds = 0\n",
    "        for idx, num_peds in zip(indices, num_peds_indices):\n",
    "            batch.append(idx)\n",
    "            total_num_peds += num_peds\n",
    "            if total_num_peds >= self.batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "                total_num_peds = 0\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        # Approximated number of batches.\n",
    "        # The order of trajectories can be shuffled, so this number can vary from run to run.\n",
    "        if self.drop_last:\n",
    "            return sum(self.data_source.num_peds_in_seq) // self.batch_size\n",
    "        else:\n",
    "            return (sum(self.data_source.num_peds_in_seq) + self.batch_size - 1) // self.batch_size\n",
    "\n",
    "\n",
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            data.append(line)\n",
    "        file = np.asarray(data)\n",
    "        new_file = file.squeeze()\n",
    "        out_put = []\n",
    "        for linE in new_file:\n",
    "            linE = linE.split()\n",
    "            linE = [float(i) for i in linE]\n",
    "            out_put.append(linE)\n",
    "        final_out = np.array(out_put)\n",
    "        return final_out\n",
    "\n",
    "\n",
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    \"\"\"Dataloder for the Trajectory datasets\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, obs_len=8, pred_len=12, skip=1, threshold=0.02, min_ped=1, delim='\\t'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        - data_dir: Directory containing dataset files in the format <frame_id> <ped_id> <x> <y>\n",
    "        - obs_len: Number of time-steps in input trajectories\n",
    "        - pred_len: Number of time-steps in output trajectories\n",
    "        - skip: Number of frames to skip while making the dataset\n",
    "        - threshold: Minimum error to be considered for non-linear traj when using a linear predictor\n",
    "        - min_ped: Minimum number of pedestrians that should be in a sequence\n",
    "        - delim: Delimiter in the dataset files\n",
    "        \"\"\"\n",
    "        super(TrajectoryDataset, self).__init__()\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.obs_len = obs_len\n",
    "        self.pred_len = pred_len\n",
    "        self.skip = skip\n",
    "        self.seq_len = self.obs_len + self.pred_len\n",
    "        self.delim = delim\n",
    "\n",
    "        all_files = os.listdir(self.data_dir)\n",
    "        all_files = [os.path.join(self.data_dir, _path) for _path in all_files]\n",
    "\n",
    "        num_peds_in_seq = []\n",
    "        seq_list = []\n",
    "        loss_mask_list = []\n",
    "        non_linear_ped = []\n",
    "        for path in all_files:\n",
    "            data = read_file(path, delim = 't')\n",
    "            frames = np.unique(data[:, 0]).tolist()\n",
    "            frame_data = []\n",
    "            for frame in frames:\n",
    "                frame_data.append(data[frame == data[:, 0], :])\n",
    "            num_sequences = len(frame_data)\n",
    "\n",
    "            for idx in range(0, num_sequences * self.skip , skip):\n",
    "                    curr_seq_data = frame_data[idx]   #np.concatenate(frame_data[idx:idx + seq_len], axis=0)\n",
    "                    first = 0\n",
    "                    end = 20\n",
    "                    frame = []\n",
    "                    curr_seq_data[first:end]\n",
    "                    for i in range(0, 30, 1):\n",
    "                        first = i\n",
    "                        end = i + 20\n",
    "                        if end > len(curr_seq_data):\n",
    "                            break\n",
    "                        frame.append(curr_seq_data[first:end])\n",
    "                    #frame\n",
    "                    #peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "                    curr_seq = np.zeros((len(frame), 2, self.seq_len))\n",
    "                    curr_loss_mask = np.zeros((len(frame), self.seq_len))\n",
    "                    num_peds_considered = 0\n",
    "                    _non_linear_ped = []\n",
    "                    for ped_id, sequence in enumerate(frame):\n",
    "                        curr_ped_seq = sequence #curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "                        curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "                        #pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "                        #pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "                        '''if pad_end - pad_front != seq_len:\n",
    "                            continue'''\n",
    "                        curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "                        curr_ped_seq = curr_ped_seq\n",
    "                        _idx = num_peds_considered\n",
    "                        curr_seq[_idx, :, :] = curr_ped_seq\n",
    "                        # Linear vs Non-Linear Trajectory\n",
    "                        _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "                        curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "                        num_peds_considered += 1\n",
    "\n",
    "                    if num_peds_considered > min_ped:\n",
    "                        non_linear_ped += _non_linear_ped\n",
    "                        num_peds_in_seq.append(num_peds_considered)\n",
    "                        loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "                        seq_list.append(curr_seq[:num_peds_considered])\n",
    "\n",
    "                \n",
    "\n",
    "        self.num_seq = len(seq_list)\n",
    "        seq_list = np.concatenate(seq_list, axis=0)\n",
    "        loss_mask_list = np.concatenate(loss_mask_list, axis=0)\n",
    "        non_linear_ped = np.asarray(non_linear_ped)\n",
    "        self.num_peds_in_seq = np.array(num_peds_in_seq)\n",
    "\n",
    "        # Convert numpy -> Torch Tensor\n",
    "        self.obs_traj = torch.from_numpy(seq_list[:, :, :self.obs_len]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.pred_traj = torch.from_numpy(seq_list[:, :, self.obs_len:]).type(torch.float).permute(0, 2, 1)  # NTC\n",
    "        self.loss_mask = torch.from_numpy(loss_mask_list).type(torch.float)\n",
    "        self.non_linear_ped = torch.from_numpy(non_linear_ped).type(torch.float)\n",
    "        cum_start_idx = [0] + np.cumsum(num_peds_in_seq).tolist()\n",
    "        self.seq_start_end = [(start, end) for start, end in zip(cum_start_idx, cum_start_idx[1:])]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_seq\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        start, end = self.seq_start_end[index]\n",
    "        out = [self.obs_traj[start:end], self.pred_traj[start:end],\n",
    "               self.non_linear_ped[start:end], self.loss_mask[start:end], None, [[0, end - start]]]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x17f4063ddf0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'D:/AmirKabir/tez/eighen trajectory/EigenTrajectory-1/datasets/nuscenes_mini'\n",
    "get_dataloader(path ,'train',12 , 8, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "def read_file(_path, delim='\\t'):\n",
    "    data = []\n",
    "    if delim == 'tab':\n",
    "        delim = '\\t'\n",
    "    elif delim == 'space':\n",
    "        delim = ' '\n",
    "    with open(_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split(delim)\n",
    "            data.append(line)\n",
    "        file = np.asarray(data)\n",
    "        new_file = file.squeeze()\n",
    "        out_put = []\n",
    "        for linE in new_file:\n",
    "            linE = linE.split()\n",
    "            linE = [float(i) for i in linE]\n",
    "            out_put.append(linE)\n",
    "        final_out = np.array(out_put)\n",
    "        return final_out\n",
    "def poly_fit(traj, traj_len, threshold):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    - traj: Numpy array of shape (2, traj_len)\n",
    "    - traj_len: Len of trajectory\n",
    "    - threshold: Minimum error to be considered for non-linear traj\n",
    "    Output:\n",
    "    - int: 1 -> Non Linear 0-> Linear\n",
    "    \"\"\"\n",
    "    t = np.linspace(0, traj_len - 1, traj_len)\n",
    "    res_x = np.polyfit(t, traj[0, -traj_len:], 2, full=True)[1]\n",
    "    res_y = np.polyfit(t, traj[1, -traj_len:], 2, full=True)[1]\n",
    "    if res_x + res_y >= threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 0.0\n",
    "path = 'D:/AmirKabir/tez/eighen trajectory/EigenTrajectory-1/datasets/nuscenes_mini/train/nuscenes_trajectory.txt'\n",
    "seq_len = 20\n",
    "skip = 1\n",
    "pred_len = 8\n",
    "threshold = 0.02\n",
    "min_ped=1\n",
    "num_peds_in_seq = []\n",
    "seq_list = []\n",
    "loss_mask_list = []\n",
    "non_linear_ped = []\n",
    "data = read_file(path, delim = 't')\n",
    "frames = np.unique(data[:, 0]).tolist()\n",
    "frame_data = []\n",
    "for frame in frames:\n",
    "    frame_data.append(data[frame == data[:, 0], :])\n",
    "num_sequences = len(frame_data)\n",
    "num_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_seq_data = frame_data[0]  \n",
    "peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "peds_in_curr_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732140e+02, 1.130480e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731830e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731520e+02, 1.130357e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731800e+02, 1.130431e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732080e+02, 1.130504e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732370e+02, 1.130577e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732580e+02, 1.130608e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732710e+02, 1.130617e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732850e+02, 1.130626e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732980e+02, 1.130635e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733110e+02, 1.130643e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733240e+02, 1.130652e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733370e+02, 1.130661e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733500e+02, 1.130670e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733630e+02, 1.130678e+03]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_ped_seq = curr_seq_data[curr_seq_data[:, 1] == 1, :]\n",
    "curr_ped_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peds_in_curr_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = 0\n",
    "end = 20\n",
    "frame = []\n",
    "curr_seq_data[first:end]\n",
    "for i in range(0, 30, 1):\n",
    "    first = i\n",
    "    end = i + 20\n",
    "    if end > len(curr_seq_data):\n",
    "        break\n",
    "    frame.append(curr_seq_data[first:end])\n",
    "frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732140e+02, 1.130480e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731830e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731520e+02, 1.130357e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731800e+02, 1.130431e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732080e+02, 1.130504e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732370e+02, 1.130577e+03]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732560e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732140e+02, 1.130480e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731830e+02, 1.130419e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731520e+02, 1.130357e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.731800e+02, 1.130431e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732080e+02, 1.130504e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732370e+02, 1.130577e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732450e+02, 1.130600e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732580e+02, 1.130608e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732710e+02, 1.130617e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732850e+02, 1.130626e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.732980e+02, 1.130635e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733110e+02, 1.130643e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733240e+02, 1.130652e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733370e+02, 1.130661e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733500e+02, 1.130670e+03],\n",
       "       [0.000000e+00, 1.000000e+00, 3.733630e+02, 1.130678e+03]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_seq_data = frame_data[0]\n",
    "curr_seq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(0, num_sequences * skip , skip):\n",
    "    curr_seq_data = frame_data[idx]   #np.concatenate(frame_data[idx:idx + seq_len], axis=0)\n",
    "    first = 0\n",
    "    end = 20\n",
    "    frame = []\n",
    "    curr_seq_data[first:end]\n",
    "    for i in range(0, 30, 1):\n",
    "        first = i\n",
    "        end = i + 20\n",
    "        if end > len(curr_seq_data):\n",
    "            break\n",
    "        frame.append(curr_seq_data[first:end])\n",
    "    #frame\n",
    "    #peds_in_curr_seq = np.unique(curr_seq_data[:, 1])\n",
    "    curr_seq = np.zeros((len(frame), 2, seq_len))\n",
    "    curr_loss_mask = np.zeros((len(frame), seq_len))\n",
    "    num_peds_considered = 0\n",
    "    _non_linear_ped = []\n",
    "    for ped_id, sequence in enumerate(frame):\n",
    "        curr_ped_seq = sequence #curr_seq_data[curr_seq_data[:, 1] == ped_id, :]\n",
    "        curr_ped_seq = np.around(curr_ped_seq, decimals=4)\n",
    "        #pad_front = frames.index(curr_ped_seq[0, 0]) - idx\n",
    "        #pad_end = frames.index(curr_ped_seq[-1, 0]) - idx + 1\n",
    "        '''if pad_end - pad_front != seq_len:\n",
    "            continue'''\n",
    "        curr_ped_seq = np.transpose(curr_ped_seq[:, 2:])\n",
    "        curr_ped_seq = curr_ped_seq\n",
    "        _idx = num_peds_considered\n",
    "        curr_seq[_idx, :, :] = curr_ped_seq\n",
    "        # Linear vs Non-Linear Trajectory\n",
    "        _non_linear_ped.append(poly_fit(curr_ped_seq, pred_len, threshold))\n",
    "        curr_loss_mask[_idx, pad_front:pad_end] = 1\n",
    "        num_peds_considered += 1\n",
    "\n",
    "    if num_peds_considered > min_ped:\n",
    "        non_linear_ped += _non_linear_ped\n",
    "        num_peds_in_seq.append(num_peds_considered)\n",
    "        loss_mask_list.append(curr_loss_mask[:num_peds_considered])\n",
    "        seq_list.append(curr_seq[:num_peds_considered])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 2, 20)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
